{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from PIL import Image \n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outroot = './out'\n",
    "#subname = os.path.join(outroot,'2019_11_03')\n",
    "path = 'trees.jpg'\n",
    "#name = 'model_trees.tar'\n",
    "name = 'model_trees.tar'\n",
    "device = 'cpu'\n",
    "#device = 'cuda:0'\n",
    "\n",
    "\"\"\"\n",
    "'epoch' 'netG_state_dict' 'netD_state_dict'\n",
    "\n",
    "'optimizerG_state_dict' 'optimizerD_state_dict'\n",
    "\n",
    "'img' 'lossG' 'lossD' 'D_x' 'D_G_z2' 'D_G_z1'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "nc = 3\n",
    "ndf = 64\n",
    "ngf = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 1, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#netG = netG = Generator().to(device)\n",
    "\n",
    "checkpoint = torch.load(name, map_location = 'cpu')\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "netD.load_state_dict(checkpoint['netD_state_dict'])\n",
    "\n",
    "netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgan(im,img):\n",
    "    if isinstance(im,np.ndarray):\n",
    "        im = torch.from_numpy(im).float()\n",
    "        im.to(device)\n",
    "        im.requires_grad = True\n",
    "\n",
    "    if isinstance(img,np.ndarray):\n",
    "        img = torch.from_numpy(img).float()\n",
    "        img.to(device)\n",
    "        img.requires_grad = True\n",
    "\n",
    "    im.requires_grad = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    return loss.detach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 1.3967e-02,  2.8264e-02, -8.3623e-03, -1.9109e-02, -4.1250e-04],\n",
       "          [ 2.0558e-02,  1.0841e-02,  1.6625e-02, -2.5326e-02, -1.3387e-02],\n",
       "          [ 2.8529e-02, -1.1748e-02,  4.3979e-03, -4.2641e-03,  1.5840e-02],\n",
       "          [ 3.0178e-02,  8.3122e-03, -5.3660e-03,  2.2779e-02, -2.2658e-02],\n",
       "          [ 1.2257e-02,  1.7058e-02,  1.0646e-02, -3.0235e-02, -2.4259e-02]],\n",
       "\n",
       "         [[ 5.4466e-03,  1.0869e-02,  1.4442e-03,  1.0135e-02,  6.5280e-03],\n",
       "          [ 7.6447e-03,  2.6189e-03,  2.1168e-02, -1.2051e-02, -4.6356e-03],\n",
       "          [-8.4321e-03,  2.3800e-02, -5.3315e-03, -7.1810e-03, -1.1322e-02],\n",
       "          [ 4.4526e-03,  3.9318e-03,  9.2283e-03, -2.1937e-02, -1.8977e-02],\n",
       "          [-2.1634e-02, -4.6305e-03,  1.2119e-02, -8.9107e-03, -1.6184e-02]],\n",
       "\n",
       "         [[-2.5128e-02, -5.9993e-04, -5.1805e-03,  8.6765e-03, -8.8936e-03],\n",
       "          [-1.0667e-04, -2.2193e-02,  9.5532e-03,  1.7776e-02, -5.8161e-03],\n",
       "          [-8.7394e-03,  5.0860e-03, -6.1039e-03, -1.0580e-02, -3.9252e-02],\n",
       "          [-6.1249e-03,  2.8316e-03,  1.7474e-02, -4.9484e-03, -2.7379e-02],\n",
       "          [ 2.8530e-02, -2.9866e-02,  1.9272e-03,  1.8908e-02,  1.6907e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.8114e-02,  2.5393e-02, -1.8524e-03,  9.1112e-03, -8.7854e-03],\n",
       "          [ 2.6882e-02,  4.6361e-03,  4.4608e-02,  9.2075e-03,  7.7990e-03],\n",
       "          [ 1.5768e-02,  1.1088e-02,  9.4452e-03,  2.9224e-02,  3.4643e-02],\n",
       "          [ 8.2448e-03,  2.4123e-02, -4.1391e-03,  2.6218e-02,  2.0795e-02],\n",
       "          [-3.9623e-03,  2.6487e-02,  1.2261e-03, -1.9196e-02, -5.6249e-03]],\n",
       "\n",
       "         [[ 3.0196e-03, -6.8292e-03, -2.9994e-03, -5.0520e-03, -1.1466e-02],\n",
       "          [ 3.1170e-02,  3.1749e-03,  2.6639e-02, -9.5545e-03, -4.9051e-02],\n",
       "          [ 1.4729e-02,  5.4117e-03,  2.3705e-02,  7.0451e-03, -1.3909e-02],\n",
       "          [ 6.0990e-03, -8.3533e-03,  5.2312e-03,  2.5575e-03, -3.3854e-02],\n",
       "          [ 1.8282e-02,  1.1303e-02,  6.5155e-02,  4.4370e-03, -8.1981e-03]],\n",
       "\n",
       "         [[ 2.5720e-02,  2.5488e-02,  4.0871e-03,  8.2995e-03, -5.9968e-03],\n",
       "          [ 1.8461e-02,  3.9190e-02,  2.6864e-04,  3.0562e-03, -2.9103e-03],\n",
       "          [ 3.3134e-02,  4.6679e-02, -6.3940e-03, -2.8473e-02, -1.6096e-02],\n",
       "          [ 4.8492e-02,  4.3404e-02,  1.7661e-02,  8.3182e-03, -1.1180e-02],\n",
       "          [ 2.9925e-02,  3.9319e-02, -1.0645e-02,  2.3111e-02,  2.3793e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.9850e-02, -1.3609e-02,  5.5707e-03, -2.9750e-02, -1.0813e-03],\n",
       "          [ 4.2093e-03, -1.1377e-02, -5.1145e-02,  1.0133e-02, -1.0804e-02],\n",
       "          [ 3.1433e-02,  2.0973e-02, -5.9366e-03, -3.8159e-02,  6.5447e-03],\n",
       "          [ 2.0140e-02, -3.2303e-03, -1.0609e-02, -1.6285e-02, -7.7468e-04],\n",
       "          [ 5.8701e-02,  1.9173e-02,  5.3022e-02, -1.9452e-03, -8.8708e-03]],\n",
       "\n",
       "         [[-1.7254e-03,  2.3533e-02,  8.6083e-03, -4.5212e-02, -1.0902e-02],\n",
       "          [ 3.1737e-02,  8.2658e-03, -4.1685e-02, -2.5300e-03,  7.6371e-03],\n",
       "          [-3.8380e-02, -2.5663e-02,  4.6981e-03,  1.3432e-02, -1.1439e-02],\n",
       "          [ 9.5208e-03, -1.1517e-02,  1.6092e-03, -1.7402e-02, -2.5046e-02],\n",
       "          [-7.0505e-03,  8.9537e-03,  5.4635e-02,  9.5071e-03,  3.1466e-02]],\n",
       "\n",
       "         [[-3.1032e-03, -1.9875e-02, -3.1500e-02,  2.0507e-02,  3.5849e-02],\n",
       "          [-3.5067e-02,  3.2812e-03,  8.0710e-02,  3.7728e-02, -6.7865e-03],\n",
       "          [ 2.5268e-02,  2.1470e-02, -1.1125e-02, -2.0873e-02,  1.7161e-02],\n",
       "          [-4.7859e-03, -1.8645e-02, -1.9505e-02,  1.1656e-02, -2.9499e-02],\n",
       "          [ 1.2097e-02,  8.9463e-03, -2.7197e-02,  1.1853e-02, -1.2679e-02]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-2.1878e-02,  3.6849e-03,  1.7597e-02, -7.9581e-03,  1.7421e-02],\n",
       "          [ 1.2321e-02,  1.6666e-02, -4.9764e-04, -2.4412e-02, -2.1081e-02],\n",
       "          [ 2.5501e-02, -3.6800e-02, -4.2409e-02, -2.7128e-02, -5.5711e-02],\n",
       "          [-2.8237e-02, -1.0161e-02, -5.8186e-02, -3.4644e-02, -1.2144e-02],\n",
       "          [-4.0738e-03, -2.0998e-02, -2.4484e-02, -1.7839e-02,  1.2594e-03]],\n",
       "\n",
       "         [[ 5.0041e-02,  2.2596e-02,  4.0330e-04, -2.0953e-02,  2.9038e-02],\n",
       "          [ 2.7270e-02,  4.4150e-04, -6.7902e-04,  1.2111e-02,  2.9998e-02],\n",
       "          [ 5.4862e-03,  1.5870e-02,  4.1733e-02,  2.7257e-02,  3.3622e-02],\n",
       "          [ 3.4416e-02,  4.7993e-02,  1.9611e-02,  2.5447e-02,  6.7524e-03],\n",
       "          [ 1.1062e-02,  3.7801e-02,  2.4361e-02,  3.9748e-02,  1.2898e-02]],\n",
       "\n",
       "         [[-1.0890e-02, -1.9859e-02,  5.5279e-03, -3.7755e-02, -5.4514e-03],\n",
       "          [ 1.9714e-02, -1.2277e-02, -2.4846e-02,  6.1989e-03, -1.3803e-02],\n",
       "          [-4.1336e-02, -1.4584e-02,  3.1889e-02,  3.8069e-03, -3.5654e-03],\n",
       "          [ 1.2362e-03,  6.6682e-03, -9.8349e-03, -5.7373e-02, -1.6215e-02],\n",
       "          [ 1.9806e-04, -6.7586e-03, -6.2615e-03, -4.0343e-02, -2.1587e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2483e-02, -4.2638e-02, -1.2989e-02, -2.8879e-02, -1.7052e-02],\n",
       "          [-1.7202e-02,  1.3666e-02, -1.3555e-02,  3.8780e-03, -2.1360e-02],\n",
       "          [ 2.0112e-02, -2.1095e-02,  3.0743e-03,  7.6743e-03, -3.1148e-02],\n",
       "          [ 3.4933e-02,  2.2274e-02, -3.8453e-02, -7.6018e-03, -1.9311e-02],\n",
       "          [ 2.4181e-02,  1.7241e-03,  2.7004e-02,  2.0986e-02, -2.0014e-02]],\n",
       "\n",
       "         [[-4.7900e-02, -4.9334e-03,  1.7468e-02, -3.5725e-02, -6.1828e-03],\n",
       "          [-1.5533e-02,  3.3358e-03, -2.3823e-02, -4.0163e-02, -1.4886e-02],\n",
       "          [-1.6225e-02, -5.9109e-03,  9.8079e-04, -2.2908e-02,  4.3350e-03],\n",
       "          [-2.6335e-03,  2.4504e-03,  1.5977e-02, -2.1989e-02, -1.6618e-02],\n",
       "          [-1.7930e-02,  1.8828e-02,  2.7972e-03,  1.1454e-02, -1.3550e-02]],\n",
       "\n",
       "         [[-6.7216e-03, -8.7865e-03,  4.9931e-03, -3.1156e-04,  1.2206e-02],\n",
       "          [ 4.6945e-03, -6.0419e-03,  1.4881e-04,  9.4261e-03,  2.0750e-02],\n",
       "          [-7.0421e-04,  1.2592e-02,  8.7013e-03, -1.2176e-02,  3.1981e-03],\n",
       "          [-6.4920e-03, -1.1770e-02,  6.6849e-03, -4.4038e-03, -1.2903e-02],\n",
       "          [ 5.1205e-02, -1.5953e-02,  1.9410e-02,  4.4935e-02,  8.6446e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.3164e-02,  7.5084e-03, -1.6050e-02,  1.6996e-02, -8.7255e-03],\n",
       "          [-6.7815e-03, -4.2219e-02, -2.4789e-02,  8.5586e-04, -1.1117e-02],\n",
       "          [-2.6397e-02, -1.4679e-02, -2.3411e-02, -3.7088e-02, -4.4742e-02],\n",
       "          [-1.5328e-02, -4.2034e-03,  3.0753e-03,  1.9444e-03, -2.4669e-02],\n",
       "          [-2.5948e-02, -1.7950e-02, -2.4787e-02, -2.6284e-02,  4.9824e-04]],\n",
       "\n",
       "         [[ 5.3882e-03,  3.3187e-02,  1.3891e-03, -1.5660e-03,  3.1818e-02],\n",
       "          [ 4.4326e-02,  4.8342e-02,  3.6136e-02,  3.0763e-02,  2.4437e-02],\n",
       "          [ 2.2076e-02,  2.8926e-02,  2.0007e-02, -7.2789e-03,  2.4412e-02],\n",
       "          [ 3.4591e-02,  4.4766e-03,  4.5397e-02,  2.9997e-02,  1.3222e-02],\n",
       "          [ 2.1030e-02,  8.2277e-03,  1.7552e-02,  7.9870e-03,  1.9693e-02]],\n",
       "\n",
       "         [[-2.2261e-02, -1.1369e-02, -1.3312e-02, -7.1860e-03, -1.3611e-02],\n",
       "          [-2.8574e-02,  6.6567e-03, -1.9980e-02, -8.9957e-04, -7.4982e-04],\n",
       "          [ 5.7234e-03, -7.1847e-03, -1.4215e-02, -2.6760e-02,  7.4404e-03],\n",
       "          [-1.6967e-05, -3.7674e-03, -3.7939e-02, -1.1571e-02,  1.6410e-02],\n",
       "          [-3.8698e-02,  4.0389e-03,  4.0453e-03, -2.2979e-02, -1.6781e-02]]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD.main[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(path)\n",
    "#im2 = cv2.imread(path)\n",
    "#plt.imshow(im)\n",
    "image_size = 256\n",
    "# convert tensor convert to channel first\n",
    "transform=transforms.Compose([\n",
    "                               #transforms.Resize(image_size),\n",
    "                               #transforms.CenterCrop(image_size),\n",
    "                               #transforms.Grayscale(num_output_channels=1),\n",
    "                               transforms.ToTensor(),\n",
    "                               #normalize#Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),    # (image - mean)/ std\n",
    "])\n",
    "\n",
    "\n",
    "source_img_org = transform(im)\n",
    "source_img_org = source_img_org.detach().numpy()\n",
    "#plt.figure()\n",
    "#plt.imshow(np.transpose((img+1)/2,(1,2,0)))\n",
    "\n",
    "#noise = torch.randn(1,nc,image_size,image_size)\n",
    "#plt.figure()\n",
    "#plt.imshow(im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(source_img_org).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "k = 6\n",
    "nc = 3\n",
    "image_height = 256\n",
    "image_width = 256\n",
    "\n",
    "noise = np.random.randn(image_height,image_width,nc)*(2**k)/255\n",
    "\n",
    "imgn = noise + source_img_org   # org RGB\n",
    "imgn = np.clip(imgn,0,1)\n",
    "\n",
    "nm1 = imagenet_mean[np.newaxis,np.newaxis,:]\n",
    "imgn = imgn[...,::-1]\n",
    "imgn = imgn - nm1\n",
    "imgn = np.transpose(imgn*255,(2,0,1))\n",
    "imgn = imgn[np.newaxis,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "img.unsqueeze_(dim = 0) \n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28915488719940186"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD(img).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 4, 5, 2, 2, 1,bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. \n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 5, 2, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.ReLU(True),\n",
    "            # state size\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf , 5, 2, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size\n",
    "            nn.ConvTranspose2d( ngf, nc, 5, 2, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 5, 2, 2, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 5, 2, 2, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 5, 2, 2, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 4, 1, 5, 2, 2, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
